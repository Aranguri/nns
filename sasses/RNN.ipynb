{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "#### Goal\n",
    "I want to focus on generalization using text for the tasks. We train modules in an unsupervised way, and we use those modules for other tasks.\n",
    "\n",
    "#### Modules\n",
    "* Measure distance between documents\n",
    "* Predict next word\n",
    "\n",
    "#### Test\n",
    "* Encode the question in a vector and use a RNN that receives that vector and outputs the answer to the question.\n",
    "\n",
    "#### Future ideas\n",
    "* Allow an increasingly bigger vocabulary: create a module emb such that emb: v -> w. Then let's define emb-1: w -> v. In matrix form $E(v) = Tv = w.$ $v = T^{-1}W$\n",
    "* use a dictionary instead of a matrix for the embeddings. (if it's more efficient.)\n",
    "* feedback after performing the action\n",
    "* three steps sigmoid\n",
    "* increasingly bigger nets\n",
    "* neuron with multiple activation functions.\n",
    "* something that lives in the internet\n",
    "* memory modules\n",
    "* other module to get good word embeddings: change the word in the middle of a sentence in half of the times to a random word.\n",
    "OTHERS: We could add a initialization for parameters as I did for basic_layers.py.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs\n",
    "Best accuracy shofar: .075 \n",
    "\n",
    "### Next steps\n",
    "* Make a jupybook roadmap explaining different steps \n",
    "* Make the loss and acc functinos smoother to observe big picture info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First module: document embeddings\n",
    "We want a nn that outputs a vector that encodes as much information as possible about a document. How do we train this module? We can't train this in a supervised way, because we don't know how each document should be encoded. However, we know that similar documents should have vectors that are near, and different documents should have vectors that are far away. \n",
    "\n",
    "So a valid loss function could be to set  \n",
    "\n",
    "TODO: try the cosine distance instead of l2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second module: predict the next word\n",
    "First step: input the first word of a text to a RNN and ask it to predict the second word.\n",
    "Second step: input the correct second word and ask it for the third word.\n",
    "Third step: ...\n",
    "\n",
    "It doesn't seem easy to predict the next word. A consistent 100% accuracy in predicting the next word means that the first word had all the information and all the next words were redundant. In fact, this would mean that there is only one valid sentence that starts with that word. And that isn't the case for any word.\n",
    "\n",
    "Let's say we start with the sentence \"A woman was\" and we are asked to predit the fourth word. There are a lot of possible words! Thus, it doesn't seem good to measure the accuracy of the rnn just taking into account one instance of the sentence \"A woman was\". In that sentence, maybe \"walking\" was the correct fourth word. But \"playing\" would have been equally valid. So, another way to measure performance could be to look for every instance of \"a woman was\" in a lot of texts and see if the word proposed by the net appears a significant amount of times after \"a woman was\". This seems costly.\n",
    "\n",
    "## Encoding topic\n",
    "Other option is to reduce the possible sentences the rnn can be talking about. Let's say we tell the rnn that it has to talk about \"a person that saw a thunderbolt when she was reading a book.\" Thus, the possible fourth words reduces. Now, asking the rnn to predict the word \"reading\" after the sentence \"a woman was\" seems much more reasonable.  \n",
    "\n",
    "Now, how do we tell the rnn the topic? First, we need to encode it. We could take the sentence we want the rnn to say and use a cnn to encode it in a vector space. Then, we set the vector as the initial hidden state of the rnn. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing some outputs\n",
    "\n",
    "### First output\n",
    "This is almost random words. The words don't even make sense locally: give went, went respiration, respiration neologism, neologism milan, church step. \n",
    "> equality complications rice flying weeks stuff slava stack why weapons liberally favored expressing give went respiration neologism milan coexist couldn church step worrying conference thanksgivings ostentation boys presentations supermarkets violence exposed difficult mild connect winning obligation distracting competes interviews consumes fixing rome fancy heat involve unintentional developed willing fallen just replied courses overhire clearing fail annoys cselle pete gamely designed vertical traction established conceptions children becomes overrate uncertain effectiveness year gained pouring cmu imagination anti kleptocracies trough decided sprawl trap schlep nexus airtight os businesses marginal breathe hotel complications daniel clouds nature shocking slowness ycfounders example brownleee salesman yourself subcomponents\n",
    "\n",
    "### Output after some training\n",
    "It's interesting to notice that it learned to output punctuation marks in a somewhat balanced distribution. Also, some pairs of consequtive words make sense: that causes, causes been, been ones, ones of, of much, much to, to death.\n",
    "\n",
    "    details also they new is harder start a be you i how \" i two super ] work right stage old \n",
    "    things over ; lead 't are by think simply how \n",
    "    swearword they i to know are you both else so startup \n",
    "    be is . there . a precarious maths round yahoo rephrase are definitely , that causes been ones of much to \n",
    "    death . out , all that , . one is you you alto about i have they longer get serve right the angels term more from \n",
    "    signing build is people i were the\n",
    "\n",
    "### Other output after some training\n",
    "Note how the nn repeated `to`, `is`, `the`, and `but`. One theory is that, as it can't be sure it will remember to output `is` after `chronic`, it overcompensates to be really sure. But now, after outputing `is`, the hidden state isn't that different, so it can't help but to output `is` again.\n",
    "\n",
    "    yahoo put portland there brushed a . the , colloquial the \n",
    "    excite that fast the \n",
    "    to to do and resist \n",
    "    good never any the you more \n",
    "    some the and think working lieu hacker at about jobs city because can the the angels , process chronic is is and \n",
    "    . maybe just \n",
    "    immunity don use refuse \n",
    "    kind if you starved tell of 't idea what bet it to insults and in they outcome . in but .  but . ? ) some buwhen the an but there the\n",
    "    way the wouldn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "[1] This could be useful for compressing. Instead of sending the whole text, we send what the text is about and in the receptor computer we have a rnn that knows how to write given an input (ie it knows grammar.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
