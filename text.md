# Next
Visualize the error surface of a neural net with two inputs and several layers. Is there one local minimum? What's the difference between the local minima? Is this representative of a bigger neural net?
No free lunch theorem. Dropout. Efficient BackProp. Practical Recommendations for Gradient-Based Training of Deep Architectures from Yoshua Bengio. Keon awesome nlp

# ideas
sum([s for s in ['a', 'b', 'c']]) could work
Apply attention in a recursive way. That is, use attention to decide where to use attention to decide where to ... and so on.
There is information we are not using with word embebida that Humans use. Namely, when we see trees and we already knew about tree, we transfer the information